{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b3d325",
   "metadata": {},
   "source": [
    "# <u>CLASSIFER TESTS NOTEBOOK</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1737750d",
   "metadata": {},
   "source": [
    "# Global libraries and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e86123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyts\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, plot_confusion_matrix, cohen_kappa_score, roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b5908",
   "metadata": {},
   "outputs": [],
   "source": [
    "BINARY_CLASSIFICATION = True\n",
    "RESPONDERS_ONLY = True\n",
    "RESPONDERS = [8,15,18,22,23,24]\n",
    "ANIMAL_LEVEL_SPLIT = True\n",
    "CATCH_22_FEATURES = False # if False, then default hand-crafted features are used\n",
    "BATCH = False\n",
    "TEST_ON_TRAIN_DATA = False # Test classifiers on training data to check for overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf584c70",
   "metadata": {},
   "source": [
    "# Fitting classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeea546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "''' Undersampling with Tomek-links ('under') or oversampling with SMOTE algorithm ('over')'''\n",
    "def resampler(X, y, ratio=0.9, resampling_type = 'over'):\n",
    "    if resampling_type == 'over':\n",
    "        resampler = SMOTE(sampling_strategy=0.9)\n",
    "    elif resampling_type == 'under':\n",
    "        resampler = TomekLinks(n_jobs=-1)\n",
    "    X, y = resampler.fit_resample(X, y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6a8da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEW DATASET INITIALISATION ##\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import pickle5 as pickle\n",
    "from collections import Counter\n",
    "from combat.pycombat import pycombat\n",
    "\n",
    "\n",
    "def data_split(animal_split = False, resampling = False, resampling_ratio = 0.9, mode = 'delta_features_dataset'):\n",
    "\n",
    "    valid_channels = [str(i) for i in range(8,25,1)]\n",
    "    if RESPONDERS_ONLY:\n",
    "        valid_channels = RESPONDERS\n",
    "    files = [f for f in listdir(os.getcwd()) if isfile(join(os.getcwd(), f))]\n",
    "    if CATCH_22_FEATURES:\n",
    "        valid_ids = [id for id in valid_channels if f'c22_dataset{id}.csv' in files]\n",
    "    else:\n",
    "        valid_ids = [id for id in valid_channels if f'{mode}_{id}.csv' in files]\n",
    "\n",
    "    def unpacker(file_references, batch_effect_elimination=BATCH):\n",
    "        X = pd.DataFrame()\n",
    "        Y = pd.DataFrame()\n",
    "        for ref in file_references:\n",
    "            if CATCH_22_FEATURES:\n",
    "                x = pd.read_csv(f'c22_labelled{ref}.csv').drop(columns=['Unnamed: 0'], axis=1)\n",
    "                labels = np.load(f'raw_dataset_y_{ref}.npy')\n",
    "                y = pd.DataFrame({'y': labels})\n",
    "            else:\n",
    "                df = pd.read_csv(f'{mode}_{ref}.csv').drop(columns=['Unnamed: 0'], axis=1)\n",
    "                if mode == 'hf_features_dataset':\n",
    "                    df = df.drop(columns=['Unnamed: 0.1'], axis=1)\n",
    "                y = df[['y']]\n",
    "                x = df.drop(columns=['y'])\n",
    "             \n",
    "            X = X.append(x)\n",
    "            Y = Y.append(y)\n",
    "        if batch_effect_elimination:\n",
    "            X = pycombat(X.drop(columns=['batch']).transpose(), X['batch']).transpose()\n",
    "        else:\n",
    "            if all(item in X.columns for item in ['batch']):\n",
    "                X = X.drop(columns=['batch'])\n",
    "        return X.reset_index(), Y.reset_index()  \n",
    "\n",
    "    data_train, data_test = train_test_split(valid_ids, test_size=0.1)\n",
    "    X_train, y_train = unpacker(data_train)\n",
    "    X_test, y_test = unpacker(data_test, False)\n",
    "    X_train, X_test = X_train.drop(columns=['index']), X_test.drop(columns=['index'])\n",
    "    y_train, y_test = np.array(y_train['y']), np.array(y_test['y'])\n",
    "    X = X_train.append(X_test)\n",
    "    y = np.array(list(y_train) + list(y_test))\n",
    "\n",
    "    if BINARY_CLASSIFICATION:\n",
    "        y[y==2] = 0\n",
    "        y_train[y_train==2] = 0\n",
    "        y_test[y_test==2] = 0\n",
    "        \n",
    "    # Resampling\n",
    "    if resampling:\n",
    "        X_train, y_train = resampler(X_train, y_train, ratio=resampling_ratio)\n",
    "        \n",
    "    if not(animal_split):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "        \n",
    "    print(f'label distribution: {Counter(y_train)}')\n",
    "    if TEST_ON_TRAIN_DATA:\n",
    "        return X_train, X_train, y_train, y_train\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da85af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "def fit_classifier(clf, num_trials, animal_split=False, resampling=False, resampling_ratio=0.9):\n",
    "    scores = []\n",
    "    roc_scores = []\n",
    "    print(f'Fitting {clf.__class__.__name__}...')\n",
    "    \n",
    "    for i in range(num_trials):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = data_split(animal_split=animal_split, resampling=resampling, resampling_ratio = resampling_ratio)\n",
    "            \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        score = np.mean((y_pred==y_test))\n",
    "        kappa = cohen_kappa_score(y_pred, y_test)\n",
    "        roc_score = None\n",
    "        if BINARY_CLASSIFICATION:\n",
    "            try:\n",
    "                roc_score = roc_auc_score(y_test, y_pred)\n",
    "                roc_scores.append(roc_score)\n",
    "            except ValueError as ve:\n",
    "                roc_score = 0.5\n",
    "        \n",
    "        print(f'y_pred: {y_pred}, actual: {y_test}, score: {score}, kappa score: {kappa}, roc_auc: {roc_score}')\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        if BINARY_CLASSIFICATION:\n",
    "            class_names = ['no decrease', 'decrease']\n",
    "        else:\n",
    "            class_names = ['stable', 'decrease', 'increase']\n",
    "        try:\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            fig, ax = plot_confusion_matrix(cm,\n",
    "                                    show_absolute=True,\n",
    "                                    colorbar=True,\n",
    "                                    class_names=class_names)\n",
    "            plt.show()\n",
    "\n",
    "            if hasattr(clf, 'feature_importances_'):\n",
    "                feat_importances = pd.Series(clf.feature_importances_, index=X_train.columns)[:10]\n",
    "                feat_importances.sort_values(inplace=True)\n",
    "                feat_importances.plot(kind='barh')\n",
    "                plt.xlabel('Fractional importance')\n",
    "                plt.ylabel('Features')\n",
    "                plt.show()\n",
    "        except AssertionError as ae:\n",
    "            print(f'Warning: No confusion matrix possible - {ae}')\n",
    "            \n",
    "        scores.append(score)\n",
    "        \n",
    "    print('_____________________________________________________________________________________ \\n')\n",
    "    print(f'{clf.__class__.__name__} accuracy: {np.mean(scores)} (+/- {2*np.std(scores)})')\n",
    "    print(f'{clf.__class__.__name__} roc_auc_accuracy: {np.mean(roc_scores)} (+/- {2*np.std(roc_scores)})')\n",
    "    print('_____________________________________________________________________________________ \\n')\n",
    "    if BINARY_CLASSIFICATION:\n",
    "        return np.mean(roc_scores)\n",
    "    print(f'SCORING:: {scores}')\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d3fead",
   "metadata": {},
   "source": [
    "# Model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23bcaec",
   "metadata": {},
   "source": [
    "## Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de04b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "TEST_ON_TRAIN_DATA = False\n",
    "\n",
    "space = {'criterion': hp.choice('criterion', ['entropy']),\n",
    "        'max_depth': hp.choice('max_depth', [3*i for i in range(1,40,1)]),\n",
    "        'max_features': hp.choice('max_features', ['auto']),\n",
    "        'min_samples_leaf': hp.uniform ('min_samples_leaf', 0, 0.1),\n",
    "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 0.5),\n",
    "        'n_estimators' : hp.choice('n_estimators', [2, 3, 5, 7, 9]) #[50, 100, 500,1000,1500,2000,5000])\n",
    "    }\n",
    "\n",
    "def objective(space):\n",
    "    model = ExtraTreesClassifier(criterion = space['criterion'], \n",
    "                                   max_depth = space['max_depth'],\n",
    "                                 max_features = space['max_features'],\n",
    "                                 min_samples_leaf = space['min_samples_leaf'],\n",
    "                                 min_samples_split = space['min_samples_split'],\n",
    "                                 n_estimators = space['n_estimators'], \n",
    "                                 n_jobs = -1\n",
    "                                 #class_weight = 'balanced'\n",
    "                                 )\n",
    "    accuracy = fit_classifier(model, num_trials = 5, animal_split = ANIMAL_LEVEL_SPLIT, resampling=False)\n",
    "\n",
    "    # We aim to maximize accuracy, therefore we return it as a negative value\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK }\n",
    "    \n",
    "trials = Trials()\n",
    "best_params = fmin(fn= objective,\n",
    "            space= space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals = 150,\n",
    "            trials= trials)\n",
    "\n",
    "\n",
    "print(best_params)\n",
    "'''\n",
    "HANDPICKED NON-RESPONDERS best loss: -0.6107236043297425]\n",
    "{'criterion': 0, 'max_depth': 15, 'max_features': 0, 'min_samples_leaf': 0.012750661927616893, 'min_samples_split': 0.007730493536295527, 'n_estimators': 3}\n",
    "NON-RESPONDERS:  best loss: -0.6327777777777778]\n",
    "{'criterion': 0, 'max_depth': 21, 'max_features': 0, 'min_samples_leaf': 0.013918505198977742, 'min_samples_split': 0.03295160386254649, 'n_estimators': 5}\n",
    "CATCH 22 NON-RESPONDERS best loss: -0.566 (AUC-ROC)\n",
    "{'criterion': 0, 'max_depth': 5, 'max_features': auto, 'min_samples_leaf': 0.07257100898991854, 'min_samples_split': 0.3850081744499732, 'n_estimators': 0}\n",
    "CATCH 22 NON-REPONDERS BALANCED, best loss: -0.6425280033975685]\n",
    "{'criterion': 0, 'max_depth': 26, 'max_features': 0, 'min_samples_leaf': 0.05946724515424022, 'min_samples_split': 0.2661228299401958, 'n_estimators': 4}\n",
    "CATCH 22 NON-REPONDERS batch, best loss: -0.5494117647058824]\n",
    "{'criterion': 0, 'max_depth': 21, 'max_features': 0, 'min_samples_leaf': 0.057393489718521554, 'min_samples_split': 0.4989641086718188, 'n_estimators': 2}\n",
    "CATCH 22 NON-RESPONDERS 3 CLASS, best loss: -0.46224270353302616]\n",
    "{'criterion': 0, 'max_depth': 3, 'max_features': 0, 'min_samples_leaf': 0.08828795232641648, 'min_samples_split': 0.29518178292497504, 'n_estimators': 0}\n",
    "HF_NON-RESPONDERS best loss: -0.7184294871794872, 2.05s/trial] (+/- 0.2716701152961734)\n",
    "{'criterion': 0, 'max_depth': 11, 'max_features': 0, 'min_samples_leaf': 0.0006541652174175058, 'min_samples_split': 0.39663870497638093, 'n_estimators': 0}\n",
    "HF COMBINED NON-RESPONDERS BALANCED, best loss: -0.7067857142857142] (+/- 0.15999299827053376) 3.46s/trial\n",
    "{'criterion': 0, 'max_depth': 25, 'max_features': 0, 'min_samples_leaf': 0.023040599674842968, 'min_samples_split': 0.22824212025886656, 'n_estimators': 3}\n",
    "*HF NON-REPONDERS UNDERSAMPLED, best loss: -0.596160372194855]\n",
    "{'criterion': 0, 'max_depth': 6, 'max_features': 0, 'min_samples_leaf': 0.048762422145537894, 'min_samples_split': 0.22400537438735946, 'n_estimators': 1}\n",
    "HF NON-REPSONDES OVERSAMPLED, best loss: -0.6126798201798201]\n",
    "{'criterion': 0, 'max_depth': 5, 'max_features': 0, 'min_samples_leaf': 0.009882907429288226, 'min_samples_split': 0.493444397300387, 'n_estimators': 4}\n",
    "HF NON-RESPONDERS THREE CLASS, best loss: -0.4469868698129568 \n",
    "{'criterion': 0, 'max_depth': 17, 'max_features': 0, 'min_samples_leaf': 0.058645900085625126, 'min_samples_split': 0.058978447877741494, 'n_estimators': 0}\n",
    "HF NON-RESPONDERS BATCH best loss: -0.6337142857142858]\n",
    "{'criterion': 0, 'max_depth': 2, 'max_features': 0, 'min_samples_leaf': 0.01035078623841787, 'min_samples_split': 0.1790916569652849, 'n_estimators': 4}\n",
    "DELTA NON-RESPONDERS, 3.53s/trial, best loss: -0.5671292759528054 (+/- 0.1128157009808595) \n",
    "{'criterion': 0, 'max_depth': 9, 'max_features': 0, 'min_samples_leaf': 0.03183344667522384, 'min_samples_split': 0.08817323561060117, 'n_estimators': 2}\n",
    "\n",
    "RESPONDERS best loss: -0.6736842105263159 (AUC-ROC)\n",
    "{'criterion': 0, 'max_depth': 45, 'max_features': 0, 'min_samples_leaf': 0.025281111352832085, 'min_samples_split': 0.002607202794544773, 'n_estimators': 1000}\n",
    "RESPONDERS CATCH22, best loss: -0.7131578947368421 (+/- 0.237950622220154) 6.09s/trial]\n",
    "{'criterion': 0, 'max_depth': 14, 'max_features': 0, 'min_samples_leaf': 0.017197968066777133, 'min_samples_split': 0.07833535344969539, 'n_estimators': 2}\n",
    "RESPONDERS CATCH22 3 CLASS, best loss: -0.3772993311036789]\n",
    "{'criterion': 0, 'max_depth': 20, 'max_features': 0, 'min_samples_leaf': 0.08720910431480049, 'min_samples_split': 0.46357970569054646, 'n_estimators': 4}\n",
    "RESPONDERS CATCH22 UNDERSAMPLED, best loss: -0.6078947368421053]\n",
    "{'criterion': 0, 'max_depth': 20, 'max_features': 0, 'min_samples_leaf': 0.040662203381250255, 'min_samples_split': 0.1989744013792977, 'n_estimators': 0}\n",
    "RESPONDERS CATCH22 OVERSAMPLED, best loss: -0.6028070175438597]\n",
    "{'criterion': 0, 'max_depth': 26, 'max_features': 0, 'min_samples_leaf': 0.06021740136878643, 'min_samples_split': 0.24842274951983306, 'n_estimators': 3}\n",
    "RESPONDERS SMALL N_ESTIMATORS CATCH 22, best loss: -0.5947368421052632]\n",
    "{'criterion': 0, 'max_depth': 29, 'max_features': 0, 'min_samples_leaf': 0.06063902001742095, 'min_samples_split': 0.3786259278204676, 'n_estimators': 3}\n",
    "RESPONDERS CATCH22 batch: best loss: -0.575]\n",
    "{'criterion': 0, 'max_depth': 10, 'max_features': 0, 'min_samples_leaf': 0.017880637004093616, 'min_samples_split': 0.11491483983622097, 'n_estimators': 4}\n",
    "RESPONDERS DELTA: best loss: -0.6103695513293037]\n",
    "{'criterion': 0, 'max_depth': 15, 'max_features': 0, 'min_samples_leaf': 0.05163641743321325, 'min_samples_split': 0.19279648497935287, 'n_estimators': 2}\n",
    "HF REPONDERS, 3.22s/trial, best loss: -0.65] (+/- 0.18708286933869706)\n",
    "{'criterion': 0, 'max_depth': 13, 'max_features': 0, 'min_samples_leaf': 0.07992646992678645, 'min_samples_split': 0.38119863891953715, 'n_estimators': 2}\n",
    "HF RESPONDERS BALANCED,  best loss: -0.6326315789473684]\n",
    "{'criterion': 0, 'max_depth': 37, 'max_features': 0, 'min_samples_leaf': 0.0622173004424529, 'min_samples_split': 0.20279857485589373, 'n_estimators': 3}\n",
    "HF COMBINED RESPONDERS BALANCED, 2.89s/trial best loss: -0.6637719298245613] (+/- 0.29022979401386984)\n",
    "{'criterion': 0, 'max_depth': 32, 'max_features': 0, 'min_samples_leaf': 0.051975911316321094, 'min_samples_split': 0.2249282596912486, 'n_estimators': 0}\n",
    "HF RESPONDERS THREE CLASS, best loss: -0.4869565217391304]\n",
    "{'criterion': 0, 'max_depth': 35, 'max_features': 0, 'min_samples_leaf': 0.00570874876573191, 'min_samples_split': 0.0825394982567266, 'n_estimators': 0}\n",
    "HF RESPONDERS BATCH, best loss: -0.6008333333333333 \n",
    "{'criterion': 0, 'max_depth': 21, 'max_features': 0, 'min_samples_leaf': 0.02745919221866434, 'min_samples_split': 0.12459145824798322, 'n_estimators': 1}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4b2bbf",
   "metadata": {},
   "source": [
    "## Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55afbb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "TEST_ON_TRAIN_DATA = False\n",
    "\n",
    "space = {'criterion': hp.choice('criterion', ['entropy']),\n",
    "        'max_depth': hp.choice('max_depth', [3*i for i in range(1,20,1)]),\n",
    "        'max_features': hp.choice('max_features', ['auto']),\n",
    "        'min_samples_leaf': hp.uniform ('min_samples_leaf', 0, 0.1),\n",
    "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 0.5),\n",
    "        'n_estimators' : hp.choice('n_estimators',  [1500,2000])# [2, 3, 5, 7, 9])  # [500,1000,1500,2000,5000]\n",
    "    }\n",
    "\n",
    "def objective(space):\n",
    "    model = RandomForestClassifier(criterion = space['criterion'], \n",
    "                                   max_depth = space['max_depth'],\n",
    "                                 max_features = space['max_features'],\n",
    "                                 min_samples_leaf = space['min_samples_leaf'],\n",
    "                                 min_samples_split = space['min_samples_split'],\n",
    "                                 n_estimators = space['n_estimators'], \n",
    "                                 n_jobs = -1\n",
    "                                 )\n",
    "    accuracy = fit_classifier(model, num_trials = 5, animal_split = ANIMAL_LEVEL_SPLIT, resampling=False)\n",
    "\n",
    "    # We aim to maximize accuracy, therefore we return it as a negative value\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK }\n",
    "    \n",
    "trials = Trials()\n",
    "best_params = fmin(fn= objective,\n",
    "            space= space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals = 100,\n",
    "            trials= trials)\n",
    "print(best_params)\n",
    "\n",
    "'''-\n",
    "[2, 3, 5, 7, 9])   [500,1000,1500,2000,5000]\n",
    "{'criterion': 0, 'max_depth': 2, 'max_features': 0, 'min_samples_leaf': 0.05503690384288085, 'min_samples_split': 0.031076491313923316, 'n_estimators': 3}\n",
    "HANDPICKED NON RESPONDERS, best loss: -0.6274437229437231 (+/- 0.15661955752871673) 15.87s/trial]\n",
    "{'criterion': 0, 'max_depth': 15, 'max_features': 0, 'min_samples_leaf': 0.01625764740377511, 'min_samples_split': 0.2964956982880342, 'n_estimators': 3}\n",
    "HANDPICKED NON RESPONDERS SMALL FOREST 2.10s/trial, best loss: -0.5914296814296816]\n",
    "{'criterion': 0, 'max_depth': 12, 'max_features': 0, 'min_samples_leaf': 0.06535997213876292, 'min_samples_split': 0.24805911299808792, 'n_estimators': 2}\n",
    "HANDPICKED RESPONDERS (Overprediction of decrease), 16.90s/trial, best loss: -0.5]\n",
    "{'criterion': 0, 'max_depth': 0, 'max_features': 0, 'min_samples_leaf': 0.00705758017682645, 'min_samples_split': 0.266104692034989, 'n_estimators': 4}\n",
    "SMALL FOREST CATCH22 RESPONDERS, best loss: -0.55]\n",
    "{'criterion': 0, 'max_depth': 14, 'max_features': 0, 'min_samples_leaf': 0.056538518170784285, 'min_samples_split': 0.10820256145213225, 'n_estimators': 4} \n",
    "HF RESPONDERS, best loss: -0.6325000000000001 (+/- 0.13605554421305702) 1.71s/trial\n",
    "{'criterion': 0, 'max_depth': 17, 'max_features': 0, 'min_samples_leaf': 0.07110229538963608, 'min_samples_split': 0.1450084663114631, 'n_estimators': 0}\n",
    "HF RESPONDERS 3 CLASS, best loss: -0.3646739130434783]\n",
    "{'criterion': 0, 'max_depth': 17, 'max_features': 0, 'min_samples_leaf': 0.05770846252235319, 'min_samples_split': 0.4814812254247135, 'n_estimators': 0}\n",
    "HF NON-RESPONDERS, best loss: -0.6318333333333334 (+/- 0.39230430365555086) 2.10s/trial\n",
    "{'criterion': 0, 'max_depth': 10, 'max_features': 0, 'min_samples_leaf': 0.07659940578952626, 'min_samples_split': 0.0026034165908682128, 'n_estimators': 3}\n",
    "HF NON-RESPONDERS 3 CLASS best loss: -0.5205396825396825]\n",
    "{'criterion': 0, 'max_depth': 8, 'max_features': 0, 'min_samples_leaf': 0.046450367468097054, 'min_samples_split': 0.2687114323925206, 'n_estimators': 0}\n",
    "CATCH_22 RESPONDER, best loss: -0.5625438596491228]\n",
    "{'criterion': 0, 'max_depth': 15, 'max_features': 0, 'min_samples_leaf': 0.02501477632438393, 'min_samples_split': 0.17515365082535356, 'n_estimators': 0}\n",
    "CATCH 22 RESPONDER 3 CLASS, best loss: -0.40652173913043477]\n",
    "{'criterion': 0, 'max_depth': 0, 'max_features': 0, 'min_samples_leaf': 0.07574179915967928, 'min_samples_split': 0.49858316089971866, 'n_estimators': 0}\n",
    "CATCH 22 NON-RESPONDERS best loss: -0.5439655172413793]\n",
    "{'criterion': 0, 'max_depth': 9, 'max_features': 0, 'min_samples_leaf': 0.07276937585678855, 'min_samples_split': 0.29796310219729616, 'n_estimators': 1}\n",
    "CATCH 22 NON-RESPONDERS 3 CLASS, best loss: -0.4521061106873464]\n",
    "{'criterion': 0, 'max_depth': 18, 'max_features': 0, 'min_samples_leaf': 0.023363672552956134, 'min_samples_split': 0.40566774286886687, 'n_estimators': 1}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691fa451",
   "metadata": {},
   "source": [
    "## XGBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1767926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## XGBOOST ##\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# 0.50 +- 0.07\n",
    "XGB_clf = xgb.XGBClassifier(n_estimators=50, n_learning_rate=0.001, max_depth=5, sample_bytree=0.1, gamma=0.0, score_func=chi2, k=10, class_weight='balanced')\n",
    "fit_classifier(XGB_clf, num_trials = 5, animal_split = True, resampling=False)\n",
    "scores = cross_val_score(XGB_clf, X_metrics, y, scoring='accuracy', cv=10)   \n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) [XGB Classifier]\" % (scores.mean(), scores.std()))\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "TEST_ON_TRAIN_DATA = False\n",
    "RESPONDERS_ONLY = False\n",
    "\n",
    "space = {'max_depth': hp.choice('max_depth', [3*i for i in range(1,40,1)]),\n",
    "        'sample_bytree': hp.uniform ('sample_bytree', 0, 0.5), \n",
    "        'n_estimators' : hp.choice('n_estimators',  [2, 3, 5, 7, 9]), # [50, 100, 500,1000,1500,2000,5000])\n",
    "         'n_learning_rate': hp.uniform ('n_learning_rate', 0, 0.2)\n",
    "    }\n",
    "\n",
    "def objective(space):\n",
    "    model = xgb.XGBClassifier(n_estimators=space['n_estimators'], n_learning_rate=space['n_learning_rate'], max_depth=space['max_depth'], sample_bytree=space['sample_bytree'], gamma=0.0, score_func=chi2, k=10, class_weight='balanced')\n",
    "    accuracy = fit_classifier(model, num_trials = 5, animal_split = ANIMAL_LEVEL_SPLIT, resampling=False)\n",
    "\n",
    "    # We aim to maximize accuracy, therefore we return it as a negative value\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK }\n",
    "    \n",
    "trials = Trials()\n",
    "best_params = fmin(fn= objective,\n",
    "            space= space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals = 150,\n",
    "            trials= trials)\n",
    "\n",
    "\n",
    "print(best_params)\n",
    "#BEST: 0.4913157894736842 (+/- 0.2189473684210526"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02564a0",
   "metadata": {},
   "source": [
    "# Dataset imports and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091d00d7",
   "metadata": {},
   "source": [
    "## File Converter and Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e537165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sktime.transformations.panel.catch22 import Catch22\n",
    "from os.path import isfile, join\n",
    "from os import listdir, getcwd\n",
    "\n",
    "valid_nums = [i for i in range(9, 25, 1)]\n",
    "files = [f for f in listdir(getcwd()) if isfile(join(getcwd(), f))]\n",
    "valid_files = [num for num in valid_nums if f'raw_dataset_{num}.npy' in files]\n",
    "\n",
    "num = 12\n",
    "\n",
    "X = np.load(f'raw_dataset_{num}.npy', allow_pickle=True)\n",
    "y = np.load(f'raw_datasety_{num}.npy', allow_pickle=True)\n",
    "trunc = 599999\n",
    "invalid_arrays = []\n",
    "print('Truncating raw data...')\n",
    "print('X shape: ', X.shape)\n",
    "X_new = np.zeros((len(X),trunc))\n",
    "for i in range(len(X)):\n",
    "    if len(X[i]) >= trunc:\n",
    "        X_new[i] = X[i][:trunc]\n",
    "    else:\n",
    "        X_new[i] = np.zeros(trunc)\n",
    "        invalid_arrays.append(i)\n",
    "        \n",
    "# Removing invalid subarrays   \n",
    "print(' Invalid (shorter than required for uniform dataset) indices in raw_dataset: ', invalid_arrays)\n",
    "X_temp = list(X_new)\n",
    "for ele in sorted(invalid_arrays, reverse = True):\n",
    "    del X_temp[ele]\n",
    "X = np.array(X_temp)\n",
    "print('Final X shape: ', X.shape)\n",
    "y = np.delete(y, invalid_arrays)\n",
    "np.save(f'raw_dataset_y_{num}.npy', y)\n",
    "\n",
    "# Catch22 transformation\n",
    "print('Transforming...')\n",
    "catch22 = Catch22(n_jobs=-1)\n",
    "catch22.fit(X)\n",
    "X_t = catch22.transform(X)\n",
    "X_t.to_csv(f'c22_dataset{num}.csv')\n",
    "print('Transformed file saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5400fb9f",
   "metadata": {},
   "source": [
    "## High frequency data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ebcea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HIGH FREQUENCY DATA PROCESSING SPIKE FREQUENCY\n",
    "\n",
    "from os.path import isfile, join\n",
    "from os import listdir, getcwd\n",
    "from delta_feature_extractor import *\n",
    "\n",
    "valid_nums = [i for i in range(8, 25, 1)]\n",
    "files = [f for f in listdir(getcwd()) if isfile(join(getcwd(), f))]\n",
    "valid_files = [num for num in valid_nums if f'hf_features_dataset_{num}.csv' in files]\n",
    "MODE = 'CATCH22' # HANDPICKED or CATCH22\n",
    "features_to_extract = [ Mean , Max , Min , MAV , Var , StD , WL , Energy_from_fft , Kurtosis , \\\n",
    "                           Skewness , Signal_Power , Signal_Energy , Min_max_difference , Wilson_Amplitude , \\\n",
    "                           Root_Mean_Square , V3  , DABS , Maximum_fractal_length , Myopulse_percentage_rate , \\\n",
    "                           Mean_Frequency ]\n",
    "for num in valid_files:\n",
    "    df = pd.read_csv(f'hf_features_dataset_{num}.csv')\n",
    "\n",
    "    glucose_indices = []\n",
    "    for i,v in enumerate(list(df['change_label'])):\n",
    "        if not np.isnan(v):\n",
    "            glucose_indices.append(i)\n",
    "    print(glucose_indices)\n",
    "    output_array = []\n",
    "    labels = []\n",
    "    for i,v in enumerate(glucose_indices):\n",
    "        if i == 0:\n",
    "            windowed_neural_df = df.iloc[0:v-1]     \n",
    "        else:\n",
    "            windowed_neural_df = df.iloc[glucose_indices[i-1]+1:v-1]\n",
    "            \n",
    "        cleaned_array = [x for x in list(windowed_neural_df['fr']) if not np.isnan(x)]\n",
    "        output_array.append(cleaned_array)\n",
    "        labels.append(df.iloc[v]['change_label'])\n",
    "        \n",
    "    if MODE == 'CATCH22':\n",
    "        catch22 = Catch22(n_jobs=-1)\n",
    "        catch22.fit(np.array(output_array))\n",
    "        X_t = catch22.transform(X)\n",
    "        X_t.to_csv(f'hf_c22_dataset{num}.csv')\n",
    "    else:\n",
    "        final_df = Feature_extraction_windowed(output_array, features_to_extract)\n",
    "        final_df['y'] = labels\n",
    "        final_df.to_csv(f'hf_dataset_{num}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb9a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HIGH FREQUENCY DATA PROCESSING SPIKE FREQUENCY and AMPLITUDE\n",
    "\n",
    "from os.path import isfile, join\n",
    "from os import listdir, getcwd\n",
    "from feature_extractor_hf import *\n",
    "\n",
    "valid_nums = [i for i in range(8, 25, 1)]\n",
    "files = [f for f in listdir(getcwd()) if isfile(join(getcwd(), f))]\n",
    "valid_files = [num for num in valid_nums if f'hf_features_dataset_{num}.csv' in files]\n",
    "MODE = 'HANDPICKED' # HANDPICKED or CATCH22\n",
    "features_to_extract = [ Mean , Max , Min , MAV , Var , StD , WL , Energy_from_fft , Kurtosis , \\\n",
    "                           Skewness , Signal_Power , Signal_Energy , Min_max_difference , Wilson_Amplitude , \\\n",
    "                           Root_Mean_Square , V3  , DABS , Maximum_fractal_length , Myopulse_percentage_rate , \\\n",
    "                           Mean_Frequency ]\n",
    "for num in valid_files:\n",
    "    df = pd.read_csv(f'hf_{num}.csv')\n",
    "\n",
    "    glucose_indices = []\n",
    "    for i,v in enumerate(list(df['change_label'])):\n",
    "        if not np.isnan(v):\n",
    "            glucose_indices.append(i)\n",
    "    print(glucose_indices)\n",
    "    output_fr_array = []\n",
    "    output_amp_array = []\n",
    "    labels = []\n",
    "    for i,v in enumerate(glucose_indices):\n",
    "        if i == 0:\n",
    "            windowed_neural_df = df.iloc[0:v-1]     \n",
    "        else:\n",
    "            windowed_neural_df = df.iloc[glucose_indices[i-1]+1:v-1]\n",
    "            \n",
    "        cleaned_fr_array = [x for x in list(windowed_neural_df['fr']) if not np.isnan(x)]\n",
    "        cleaned_amp_array = [x for x in list(windowed_neural_df['amp']) if not np.isnan(x)]\n",
    "        output_fr_array.append(cleaned_fr_array)\n",
    "        output_amp_array.append(cleaned_amp_array)\n",
    "        labels.append(df.iloc[v]['change_label'])\n",
    "        \n",
    "    if MODE == 'CATCH22':\n",
    "        catch22 = Catch22(n_jobs=-1)\n",
    "        catch22.fit(np.array(output_array))\n",
    "        X_t = catch22.transform(X)\n",
    "        X_t.to_csv(f'hf_c22_dataset{num}.csv')\n",
    "    else:\n",
    "        final_df = Feature_extraction_windowed(output_fr_array, output_amp_array, features_to_extract)\n",
    "        final_df['y'] = labels\n",
    "        final_df = final_df.drop(columns=['Mean', 'Max', 'Min', 'MAV', 'Var', 'StD', 'WL',\n",
    "       'Energy_from_fft', 'Kurtosis', 'Skewness', 'Signal_Power',\n",
    "       'Signal_Energy', 'Min_max_difference', 'Wilson_Amplitude',\n",
    "       'Root_Mean_Square', 'V3', 'DABS', 'Maximum_fractal_length',\n",
    "       'Myopulse_percentage_rate', 'Mean_Frequency'])\n",
    "        final_df.to_csv(f'hf_combined_dataset_{num}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd83677",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 9\n",
    "#X = np.load(f'hf_windowedX_{num}.npy')\n",
    "X = pd.read_csv(f'hf_combined_dataset_9.csv')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3aa342",
   "metadata": {},
   "source": [
    "## Delta data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85110129",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DELTA DATASETS GENERATION ##\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sktime.transformations.panel.catch22 import Catch22\n",
    "from os.path import isfile, join\n",
    "from os import listdir, getcwd\n",
    "\n",
    "valid_nums = [i for i in range(8, 25, 1)]\n",
    "files = [f for f in listdir(getcwd()) if isfile(join(getcwd(), f))]\n",
    "valid_files = [num for num in valid_nums if f'raw_dataset_{num}.npy' in files]\n",
    "\n",
    "for num in valid_files:\n",
    "    X = np.load(f'raw_dataset_{num}.npy', allow_pickle=True)\n",
    "    y = np.load(f'raw_datasety_{num}.npy', allow_pickle=True)\n",
    "    window = 1000\n",
    "    invalid_arrays = []\n",
    "    print('Truncating raw data...')\n",
    "    print('X shape: ', X.shape)\n",
    "    X_new = np.zeros((len(X),window))\n",
    "    for i in range(len(X)):\n",
    "        if len(X[i]) >= 2*window:\n",
    "            X_new[i] = X[i][-window:] - X[i][:window]\n",
    "        else:\n",
    "            X_new[i] = np.zeros(window)\n",
    "            invalid_arrays.append(i)\n",
    "\n",
    "    # Removing invalid subarrays   \n",
    "    print(' Invalid (shorter than required for uniform dataset) indices in raw_dataset: ', invalid_arrays)\n",
    "    X_temp = list(X_new)\n",
    "    for ele in sorted(invalid_arrays, reverse = True):\n",
    "        del X_temp[ele]\n",
    "    X = np.array(X_temp)\n",
    "    print('Final X shape: ', X.shape)\n",
    "    y = np.delete(y, invalid_arrays)\n",
    "    np.save(f'raw_delta_dataset{num}.npy', X)\n",
    "    np.save(f'raw_delta_dataset_y_{num}.npy', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a576c94",
   "metadata": {},
   "source": [
    "## Handpicked features processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b576a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "# Add my module to python path\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from delta_feature_extractor import *\n",
    "import pickle\n",
    "from os.path import isfile, join\n",
    "from os import listdir, getcwd\n",
    "\n",
    "valid_nums = [i for i in range(8, 25, 1)]\n",
    "files = [f for f in listdir(getcwd()) if isfile(join(getcwd(), f))]\n",
    "valid_files = [num for num in valid_nums if f'hf_windowedX_{num}.npy' in files]\n",
    "\n",
    "for num in valid_files:\n",
    "\n",
    "    X = np.load(f'hf_windowedX_{num}.npy')\n",
    "    y = np.load(f'hf_windowedy_{num}.npy')\n",
    "    \n",
    "    # Feature Extraction Parameters\n",
    "    features_to_extract = [ Mean , Max , Min , MAV , Var , StD , WL , Energy_from_fft , Kurtosis , \\\n",
    "                           Skewness , Signal_Power , Signal_Energy , Min_max_difference , Wilson_Amplitude , \\\n",
    "                           Root_Mean_Square , V3  , DABS , Maximum_fractal_length , Myopulse_percentage_rate , \\\n",
    "                           Mean_Frequency ]\n",
    "\n",
    "    final_df = Feature_extraction_windowed(X, features_to_extract)\n",
    "    final_df['y'] = y\n",
    "    \n",
    "    \n",
    "    print(final_df) \n",
    "    final_df.to_csv(f'hf_features_dataset_{num}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd6bd41",
   "metadata": {},
   "source": [
    "## Batch labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f895ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "animal = {8:1, 9:2, 10:2, 11:3, 12:4, 13:4, 14:5, 15:6, 16:7, 17:8, 18:9, 19:10, 20:11, 21:11, 22:12, 23:12, 24:13}\n",
    "\n",
    "for ref in range(8,25,1):\n",
    "    \n",
    "    df = pd.read_csv(f'c22_dataset{ref}.csv').drop(columns=['Unnamed: 0'], axis=1)\n",
    "    df['batch'] = animal[ref]\n",
    "    df.to_csv(f'c22_labelled{ref}.csv')\n",
    "    \n",
    "    df = pd.read_csv(f'delta_features_{ref}.csv')\n",
    "    df['batch'] = animal[ref]\n",
    "    df.to_csv(f'delta_features_dataset_{ref}.csv')\n",
    "    \n",
    "    if ref not in [22,23,24]:\n",
    "        df = pd.read_csv(f'hf_dataset_{ref}.csv')\n",
    "        df['batch'] = animal[ref]\n",
    "        df.to_csv(f'hf_features_dataset_{ref}.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dcc77d",
   "metadata": {},
   "source": [
    "## Batch correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32377e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PYCOMBAT TESTS ##\n",
    "\n",
    "from combat.pycombat import pycombat\n",
    "\n",
    "X = pd.DataFrame()\n",
    "Y = pd.DataFrame()\n",
    "for ref in range(8,25,1):\n",
    "    x = pd.read_csv(f'c22_labelled{ref}.csv')\n",
    "    X = X.append(x)\n",
    "corrected_df = pycombat(X.drop(columns=['batch']).transpose(), X['batch']).transpose()\n",
    "print(corrected_df.drop(columns=['Unnamed: 0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97b85f1",
   "metadata": {},
   "source": [
    "# Alternate feature transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc953ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "import tsfresh\n",
    "extracted_features = impute(pd.read_csv('featuresX200.csv'))[:-1]\n",
    "#extracted_features.drop(index=extracted_features.index[0], axis=0, inplace=True)\n",
    "extracted_features = extracted_features.reset_index() #788 features\n",
    "y = pd.read_csv('featuresy200.csv')['1.000000000000000000e+00']\n",
    "print(len(extracted_features.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28cab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh.feature_selection.relevance import calculate_relevance_table\n",
    "relevance_table = calculate_relevance_table(extracted_features, y, ml_task = 'classification',multiclass=True, n_jobs=2, show_warnings=True)\n",
    "#relevance_table = relevance_table[relevance_table.relevant]\n",
    "#relevance_table.sort_values(\"p_value\", ascending =False, inplace=True)\n",
    "print(relevance_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a476718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relevance_table[relevance_table.relevant])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40df4a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading data ##\n",
    "from collections import Counter\n",
    "\n",
    "X = np.load('filtered_windowed_datasetX.npy')\n",
    "y = np.load('filtered_windowed_datasety.npy')\n",
    "Trunc = 300000 # max: 599999\n",
    "\n",
    "\n",
    "THINNING_FACTOR = 1\n",
    "X_new = np.zeros((len(X),Trunc-1))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    X_new[i] = X[i][Trunc:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size = 0.15)\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "X = X_new\n",
    "print(len(X_new[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603427fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SHAPELET TRANSFORM ## ! OUT OF MEMORY ERROR NOT FEASIBLE?\n",
    "\n",
    "from pyts.transformation import ShapeletTransform\n",
    "\n",
    "transform = ShapeletTransform(sort = True, n_jobs = 6)\n",
    "transform.fit(X_new,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83351472",
   "metadata": {},
   "source": [
    "# Alternate classifier models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bae0de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ON_TRAIN_DATA = False\n",
    "\n",
    "## METRICS ENSEMBLE CLASSIFIERS (AVERAGING MODELS) ##\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "XT_clf = ExtraTreesClassifier(n_estimators=100, max_depth=5, n_jobs=-1, class_weight='balanced')\n",
    "RF_clf = RandomForestClassifier(n_estimators=100, max_depth=5, n_jobs=-1, class_weight='balanced') \n",
    "BC_clf = BaggingClassifier(ExtraTreesClassifier(n_estimators=100, max_depth=5, n_jobs=-1, class_weight='balanced'), n_jobs=-1)\n",
    "fit_classifier(XT_clf, num_trials = 5, animal_split = ANIMAL_LEVEL_SPLIT, resampling=False)\n",
    "fit_classifier(RF_clf, num_trials = 5, animal_split = ANIMAL_LEVEL_SPLIT, resampling=False)\n",
    "fit_classifier(BC_clf, num_trials = 5, animal_split = ANIMAL_LEVEL_SPLIT, resampling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acfad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RANDOM TREES EMBEDDING -> LOGISTIC REGRESSION\n",
    "\n",
    "from sklearn.ensemble import RandomTreesEmbedding\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "random_tree_embedding = RandomTreesEmbedding(\n",
    "    n_estimators=10000, n_jobs=-1\n",
    ")\n",
    "\n",
    "rt_model = make_pipeline(random_tree_embedding, ExtraTreesClassifier(n_estimators=1000, n_jobs=-1, class_weight='balanced')) \n",
    "\n",
    "fit_classifier(rt_model, num_trials = 5, animal_split = True, resampling=False)\n",
    "#rt_model.fit(X_train, y_train)\n",
    "#print(rt_model.score(X_test, y_test))\n",
    "# roc_auc_accuracy: 0.4966666666666667 (+/- 0.013333333333333286)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54869d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STACKING CLASSIFIERS ##\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=10, random_state=42, class_weight='balanced')),\n",
    "    ('svr', make_pipeline(StandardScaler(),\n",
    "    LinearSVC(random_state=42)))]\n",
    "    \n",
    "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(class_weight='balanced'))\n",
    "#scores = cross_val_score(stacking_clf, X_metrics, y, scoring='accuracy', cv=5)   \n",
    "#print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "fit_classifier(stacking_clf, num_trials = 5, animal_split = True, resampling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422e0e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## XGB GRIDSEARCH OPTIMIZATION ##\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "clf = xgb.XGBClassifier() # 0.47 +- 0.7\n",
    "search_space = [\n",
    "  {\n",
    "    'clf__n_estimators': [50, 200], #50 \n",
    "    'clf__learning_rate': [0.01],\n",
    "    'clf__max_depth': range(5, 10), #5\n",
    "    'clf__colsample_bytree': [i/10.0 for i in range(1, 3)], # 0.1\n",
    "    'clf__gamma': [i/10.0 for i in range(3)], # 0\n",
    "    'fs__score_func': [chi2],\n",
    "    'fs__k': [10]\n",
    "  }\n",
    "]# Define cross validation\n",
    "kfold = KFold(n_splits=10)# AUC and accuracy as score\n",
    "scoring = {'Accuracy':make_scorer(accuracy_score)}# Define grid search\n",
    "grid = GridSearchCV(\n",
    "  clf,\n",
    "  param_grid=search_space,\n",
    "  cv=kfold,\n",
    "  scoring=scoring,\n",
    "  refit='Accuracy',\n",
    "  verbose=1,\n",
    "  n_jobs=-1\n",
    ")# Fit grid search\n",
    "model = grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275960a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.panel.tsfresh import TSFreshFeatureExtractor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = make_pipeline(\n",
    "    TSFreshFeatureExtractor(default_fc_parameters=\"efficient\", show_warnings=False),\n",
    "    RandomForestClassifier(),\n",
    ")\n",
    "classifier.fit(X_train, y_train)\n",
    "print(classifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30142e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "#clf = GradientBoostingClassifier(learning_rate=0.001,n_estimators=10000).fit(X_train, y_train) # AVERAGE: 0.49\n",
    "HGB_clf = HistGradientBoostingClassifier(loss='categorical_crossentropy',learning_rate=0.01, max_iter=10000) # AVERAGE: 0.45 (consistent)\n",
    "fit_classifier(HGB_clf, num_trials = 5, animal_split = True, resampling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a27332",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROCKET TRANSFORM ## ! RANDOM\n",
    "\n",
    "from pyts.transformation import ROCKET\n",
    "\n",
    "rocket = ROCKET()\n",
    "rocket.fit(X_train)\n",
    "X_train_R = rocket.transform(X_train)\n",
    "X_test_R = rocket.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7064c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RIDGECLASSIFIERCV FITTING ##\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "\n",
    "classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "fit_classifier(classifier, num_trials = 5, animal_split = True, resampling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090b6cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CATCH22 CLASSIFIER ##\n",
    "\n",
    "from sktime.classification.feature_based import Catch22Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = Catch22Classifier(\n",
    "    estimator=RandomForestClassifier(n_estimators=60, n_jobs=-1), #0.63 best, 0.56 average\n",
    "    outlier_norm=True,\n",
    "    n_jobs = -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ff71f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "classifier = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=10000)\n",
    "classifier.fit(X_train_R, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94bc8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RESULTS ##\n",
    "\n",
    "print(classifier.score(X_test_R, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0141ab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MINIROCKET TRANSFORM ## !\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sktime.transformations.panel.rocket import MiniRocket\n",
    "\n",
    "minirocket = MiniRocket(n_jobs=-1) \n",
    "minirocket.fit(X)\n",
    "X_MR = minirocket.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446fa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RIDGECLASSIFIERCV FITTING ## !SGDCLASSIFIER USED FOR LARGER DATASETS (RECOMMENDED BY CREATORS)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "#print(classifier.score(X_test_transform, y_test))\n",
    "scores = cross_val_score(clf, X_MR, y, scoring='accuracy', cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7694fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PROXIMITY FORESTS ## ! OUT OF MEMORY ERROR NOT FEASIBLE?\n",
    "from sktime.classification.distance_based import ProximityForest\n",
    "\n",
    "classifier = ProximityForest(n_estimators=10, n_jobs=-1)\n",
    "fit_classifier(classifier, num_trials = 5, animal_split = True, resampling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756ef08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MRSEQL CLASSIFIER ## \n",
    "\n",
    "from sktime.classification.shapelet_based import MrSEQLClassifier\n",
    "\n",
    "classifier = MrSEQLClassifier(seql_mode='clf', symrep=['sax', 'sfa'])\n",
    "fit_classifier(classifier, num_trials = 5, animal_split = True, resampling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e4a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RESULTS ##\n",
    "\n",
    "print(classifier.score(X_test_R, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe3f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_classifier(X, y, classifier, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c197954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)\n",
    "print(classifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3537441",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VOTING ##\n",
    "from sklearn.ensemble import VotingClassifier#create a dictionary of our models\n",
    "from sklearn.linear_model import LogisticRegression#create a new logistic regression model\n",
    "from sklearn.ensemble import RandomForestClassifier#create a new random forest classifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier#create new a knn model\n",
    "\n",
    "knn = KNeighborsClassifier()#create a dictionary of all values we want to test for n_neighbors\n",
    "params_knn = {'n_neighbors': np.arange(1, 50)}#use gridsearch to test all values for n_neighbors\n",
    "knn_gs = GridSearchCV(knn, params_knn, cv=5)#fit model to training data\n",
    "knn_gs.fit(X_train, y_train)\n",
    "#save best model\n",
    "knn_best = knn_gs.best_estimator_#check best n_neigbors value\n",
    "knn_best.fit(X_train, y_train)\n",
    "print(knn_best.score(X_test, y_test))\n",
    "print(knn_gs.best_params_)\n",
    "\n",
    "'''\n",
    "rf = RandomForestClassifier()#create a dictionary of all values we want to test for n_estimators\n",
    "params_rf = {'n_estimators': [50, 100, 200]}#use gridsearch to test all values for n_estimators\n",
    "rf_gs = GridSearchCV(rf, params_rf, cv=5)#fit model to training data\n",
    "rf_gs.fit(X_train, y_train)\n",
    "#save best model\n",
    "rf_best = rf_gs.best_estimator_#check best n_estimators value\n",
    "print(rf_gs.best_params_)\n",
    "\n",
    "estimators=[('knn', knn_best), ('rf', rf_best)]#create our voting classifier, inputting our models\n",
    "ensemble = VotingClassifier(estimators, voting='hard', n_jobs=8)\n",
    "\n",
    "classifier = Catch22Classifier(\n",
    "    estimator=ensemble, \n",
    "    outlier_norm=True,\n",
    "    n_jobs = 8\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f10821",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)\n",
    "print(classifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed84064",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HIVECOTEV1 ##\n",
    "\n",
    "from sktime.classification.hybrid import HIVECOTEV1\n",
    "\n",
    "clf = HIVECOTEV1(verbose = 1, n_jobs = -1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a60f49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HIVECOTEV2 ##\n",
    "\n",
    "from sktime.classification.hybrid import HIVECOTEV2\n",
    "\n",
    "clf = HIVECOTEV2(time_limit_in_minutes = 50, verbose = 1, n_jobs = -1)\n",
    "fit_classifier(clf, num_trials = 5, animal_split = True, resampling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b736a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## KNN with DTW ## ! 0.433\n",
    "\n",
    "from tslearn.neighbors import KNeighborsTimeSeriesClassifier\n",
    "\n",
    "clf = KNeighborsTimeSeriesClassifier(n_jobs=-1)\n",
    "clf.fit(X_train,y_train)\n",
    "print(clf.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b259b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVM with JAK ##\n",
    "\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from tslearn.svm import TimeSeriesSVC\n",
    "\n",
    "#X_train_T = TimeSeriesScalerMinMax().fit_transform(X_train)\n",
    "#X_test_T = TimeSeriesScalerMinMax().fit_transform(X_test)\n",
    "\n",
    "\n",
    "clf = TimeSeriesSVC(n_jobs=-1, verbose=1)\n",
    "fit_classifier(clf, num_trials = 5, animal_split = True, resampling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2aaa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LEARNING TIME-SERIES SHAPELETS ## ! TOO MUCH MEMORY REQUIRED\n",
    "\n",
    "from pyts.classification import LearningShapelets\n",
    "\n",
    "clf = LearningShapelets(verbose=1, n_jobs=-1)\n",
    "clf.fit(X, y)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afaf6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TIME SERIES FOREST ## ! Random\n",
    "\n",
    "from pyts.classification import TimeSeriesForest\n",
    "\n",
    "clf = TimeSeriesForest(n_jobs=-1, verbose=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a09cb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WEASEL ##\n",
    "\n",
    "from pyts.transformation import WEASEL\n",
    "\n",
    "# WEASEL transformation\n",
    "weasel = WEASEL(word_size=2, n_bins=2, window_sizes=[12, 36], sparse=False)\n",
    "X_weasel = weasel.fit_transform(X_train, y_train)\n",
    "\n",
    "# Classifier \n",
    "classifier = SVC()\n",
    "classifier.fit(X_weasel, y_train)\n",
    "print(classifier.score(weasel.transform(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29532e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAXVSM ##\n",
    "\n",
    "from pyts.classification import SAXVSM\n",
    "\n",
    "clf = SAXVSM(window_size=0.5, word_size=0.5, n_bins=5, strategy='normal')\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d49eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXTRACTED ALL HANDPICKED METRICS ## ! TODO: PLOT ROC CURVES, FEATURE IMPORTANCES\n",
    "\n",
    "X_metrics = pd.read_csv('X_filtered_features.csv').drop(['Unnamed: 0'], axis=1)\n",
    "y = np.load('filtered_windowed_datasety.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ad87f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STACKED GENERALISER ##\n",
    "\n",
    "from stacked_generalizer import StackedGeneralizer\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "VERBOSE = True\n",
    "N_FOLDS = 5\n",
    "\n",
    "# define base models\n",
    "base_models = [RandomForestClassifier(n_estimators=5000, n_jobs=-1, criterion='gini'),\n",
    "               RandomForestClassifier(n_estimators=5000, n_jobs=-1, criterion='entropy'),\n",
    "               ExtraTreesClassifier(n_estimators=5000, n_jobs=-1, criterion='gini')] # AVERAGE: 0.50\n",
    "\n",
    "# define blending model\n",
    "blending_model = LogisticRegression()\n",
    "\n",
    "# initialize multi-stage model\n",
    "sg = StackedGeneralizer(base_models, blending_model, \n",
    "                    n_folds=N_FOLDS, verbose=VERBOSE)\n",
    "# fit model\n",
    "sg.fit(X_train,y_train)\n",
    "\n",
    "# test accuracy\n",
    "pred = sg.predict(X_test)\n",
    "pred_classes = [np.argmax(p) for p in pred]\n",
    "\n",
    "_ = sg.evaluate(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cda63b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## METRICS ENSEMBLE CLASSIFIERS (BOOSTING MODELS) ##\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "AB_clf = AdaBoostClassifier(SVC(probability=True, kernel='linear'), 1000)\n",
    "fit_classifier(X_metrics, y, AB_clf, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f3674",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HARD VOTING CLASSIFIER ##\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X_metrics, y, scoring='accuracy', cv=5)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    \n",
    "fit_classifier(X_metrics, y, eclf, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c17ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SOFT VOTING CLASSIFIER ##\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from itertools import product\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Training classifiers\n",
    "clf1 = DecisionTreeClassifier(max_depth=4)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=7)\n",
    "clf3 = SVC(kernel='rbf', probability=True)\n",
    "eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)], voting='soft', weights=[1, 2, 1])\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['DecisionTree', 'KNN', 'SVC', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X_metrics, y, scoring='accuracy', cv=5)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    \n",
    "fit_classifier(X_metrics, y, eclf, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c2e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(model, X_metrics, y, scoring='accuracy', cv=5)   \n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) [XGB Classifier]\" % (scores.mean(), scores.std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
